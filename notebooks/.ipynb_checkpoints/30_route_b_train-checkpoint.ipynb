{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ABOUTME: Route B training notebook for TRM with KDA layers.\n",
        "# ABOUTME: Tracks recursion settings and deep supervision steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# route b: trm + kda\n",
        "\n",
        "use this notebook for small-scale runs and recursion tuning.\n",
        "\n",
        "all model implementations are in pytorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## run checklist\n",
        "- select dataset snapshot tag and tokenizer version\n",
        "- set recursion steps and deep supervision count\n",
        "- keep kda/full attention ratio at 3:1 in the core\n",
        "- run a short smoke train and confirm loss decreases\n",
        "\n",
        "## config sketch\n",
        "- start with a small recursion depth for stability\n",
        "- log arc-agi evals each checkpoint\n",
        "- compare with route a at matched token budgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: build TRM core, set recursion steps, run a short training loop\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## model + training config (route b)\n",
        "this config targets ~100m params and uses full recursion + deep supervision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"block_size\": 1024,\n",
        "    \"n_layer\": 8,\n",
        "    \"n_head\": 12,\n",
        "    \"n_embd\": 768,\n",
        "    \"kda_chunk_size\": 64,\n",
        "    \"kda_ratio\": 3,\n",
        "    \"h_cycles\": 2,\n",
        "    \"l_cycles\": 2,\n",
        "    \"deep_steps\": 2,\n",
        "}\n",
        "\n",
        "train_config = {\n",
        "    \"batch_size\": 8,\n",
        "    \"max_steps\": 200,\n",
        "    \"max_tokens\": 200_000,\n",
        "    \"lr\": 3e-4,\n",
        "    \"log_interval\": 10,\n",
        "}\n",
        "model_config, train_config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## kda attention + trm model (route b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def chunk_kda(q, k, v, g, beta, chunk_size):\n",
        "    dtype = v.dtype\n",
        "    b, t, h, kdim = q.shape\n",
        "    vdim = v.shape[-1]\n",
        "    c = chunk_size\n",
        "    n = t // c\n",
        "    t_trunc = n * c\n",
        "    q = q[:, :t_trunc]\n",
        "    k = k[:, :t_trunc]\n",
        "    v = v[:, :t_trunc]\n",
        "    g = g[:, :t_trunc]\n",
        "    beta = beta[:, :t_trunc]\n",
        "    q = q.view(b, n, c, h, kdim).permute(0, 3, 1, 2, 4).to(torch.float32)\n",
        "    k = k.view(b, n, c, h, kdim).permute(0, 3, 1, 2, 4).to(torch.float32)\n",
        "    v = v.view(b, n, c, h, vdim).permute(0, 3, 1, 2, 4).to(torch.float32)\n",
        "    g = g.view(b, n, c, h, kdim).permute(0, 3, 1, 2, 4).to(torch.float32)\n",
        "    beta = beta.view(b, n, c, h).permute(0, 3, 1, 2).to(torch.float32)\n",
        "    q = q * (kdim ** -0.5)\n",
        "    g = g.cumsum(-2)\n",
        "    mask = torch.triu(torch.ones(c, c, dtype=torch.bool, device=q.device), diagonal=0)\n",
        "    a = torch.zeros(b, h, n, c, c, dtype=torch.float32, device=q.device)\n",
        "    for i in range(c):\n",
        "        k_i = k[..., i, :]\n",
        "        g_i = g[..., i:i+1, :]\n",
        "        a[..., i] = torch.einsum(\"... c d, ... d -> ... c\", k * (g - g_i).exp(), k_i)\n",
        "    a = a * beta[..., None]\n",
        "    a = -a.masked_fill(mask, 0)\n",
        "    for i in range(1, c):\n",
        "        a[..., i, :i] = a[..., i, :i].clone() + (a[..., i, :, None].clone() * a[..., :i]).sum(-2)\n",
        "    a = (a + torch.eye(c, dtype=torch.float32, device=q.device)) * beta[..., None, :]\n",
        "    w = torch.einsum(\"... i j, ... j d -> ... i d\", a, g.exp() * k)\n",
        "    u = torch.einsum(\"... i j, ... j d -> ... i d\", a, v)\n",
        "    s = k.new_zeros(b, h, kdim, vdim)\n",
        "    o = torch.zeros_like(v)\n",
        "    mask = torch.triu(torch.ones(c, c, dtype=torch.bool, device=q.device), diagonal=1)\n",
        "    for i in range(0, n):\n",
        "        q_i, k_i, u_i, g_i, w_i = q[:, :, i], k[:, :, i], u[:, :, i], g[:, :, i], w[:, :, i]\n",
        "        a = torch.zeros(b, h, c, c, dtype=torch.float32, device=q.device)\n",
        "        for j in range(c):\n",
        "            k_j = k[:, :, i, j]\n",
        "            g_j = g[:, :, i, j:j+1, :]\n",
        "            a[..., j] = torch.einsum(\"... c d, ... d -> ... c\", q_i * (g_i - g_j).exp(), k_j)\n",
        "        a = a.masked_fill(mask, 0)\n",
        "        v_i = u_i - torch.einsum(\"... i j, ... j d -> ... i d\", w_i, s)\n",
        "        o[:, :, i] = torch.einsum(\"... c d, ... d v -> ... c v\", q_i * g_i.exp(), s) + torch.einsum(\"... i j, ... j d -> ... i d\", a, v_i)\n",
        "        s = s * g_i[:, :, -1].exp().unsqueeze(-1)\n",
        "        s = s + torch.einsum(\"... c d, ... c v -> ... d v\", (g_i[:, :, -1:] - g_i).exp() * k_i, v_i)\n",
        "    o = o.permute(0, 2, 3, 1, 4).contiguous().view(b, t_trunc, h, vdim)\n",
        "    return o.to(dtype)\n",
        "\n",
        "class KDAAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, chunk_size):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = n_embd // n_head\n",
        "        self.chunk_size = chunk_size\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd)\n",
        "        self.g_proj = nn.Linear(n_embd, n_embd)\n",
        "        self.beta_proj = nn.Linear(n_embd, n_head)\n",
        "        self.out_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t, c = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.split(c, dim=-1)\n",
        "        q = q.view(b, t, self.n_head, self.head_dim)\n",
        "        k = k.view(b, t, self.n_head, self.head_dim)\n",
        "        v = v.view(b, t, self.n_head, self.head_dim)\n",
        "        g = -F.softplus(self.g_proj(x)).view(b, t, self.n_head, self.head_dim)\n",
        "        beta = torch.sigmoid(self.beta_proj(x)).view(b, t, self.n_head)\n",
        "        o = chunk_kda(q, k, v, g, beta, self.chunk_size)\n",
        "        o = o.view(b, t, c)\n",
        "        return self.out_proj(o)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embd, expansion=4):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_embd, expansion * n_embd)\n",
        "        self.fc2 = nn.Linear(expansion * n_embd, n_embd)\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, use_kda, chunk_size):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.attn = KDAAttention(n_embd, n_head, chunk_size) if use_kda else CausalSelfAttention(n_embd, n_head)\n",
        "        self.mlp = MLP(n_embd)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = n_embd // n_head\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd)\n",
        "        self.out_proj = nn.Linear(n_embd, n_embd)\n",
        "    def forward(self, x):\n",
        "        b, t, c = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.split(c, dim=-1)\n",
        "        q = q.view(b, t, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(b, t, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(b, t, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        mask = torch.triu(torch.ones(t, t, device=x.device), diagonal=1).bool()\n",
        "        att = att.masked_fill(mask, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(b, t, c)\n",
        "        return self.out_proj(y)\n",
        "\n",
        "class TRMLanguageModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"n_embd\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"block_size\"], cfg[\"n_embd\"])\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i in range(cfg[\"n_layer\"]):\n",
        "            use_kda = (i % (cfg[\"kda_ratio\"] + 1)) != cfg[\"kda_ratio\"]\n",
        "            self.blocks.append(Block(cfg[\"n_embd\"], cfg[\"n_head\"], use_kda, cfg[\"kda_chunk_size\"]))\n",
        "        self.ln_f = nn.LayerNorm(cfg[\"n_embd\"])\n",
        "        self.lm_head = nn.Linear(cfg[\"n_embd\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def _forward_once(self, idx, z_h, z_l):\n",
        "        b, t = idx.shape\n",
        "        pos = torch.arange(0, t, device=idx.device)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        z_l = z_l + z_h + x\n",
        "        for block in self.blocks:\n",
        "            z_l = block(z_l)\n",
        "        z_h = z_h + z_l\n",
        "        for block in self.blocks:\n",
        "            z_h = block(z_h)\n",
        "        return z_h, z_l\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        b, t = idx.shape\n",
        "        z_h = torch.zeros(b, t, self.cfg[\"n_embd\"], device=idx.device)\n",
        "        z_l = torch.zeros(b, t, self.cfg[\"n_embd\"], device=idx.device)\n",
        "        losses = []\n",
        "        for step in range(self.cfg[\"deep_steps\"]):\n",
        "            with torch.no_grad():\n",
        "                for _ in range(self.cfg[\"h_cycles\"] - 1):\n",
        "                    for _ in range(self.cfg[\"l_cycles\"]):\n",
        "                        z_l, z_h = self._forward_once(idx, z_h, z_l)\n",
        "            for _ in range(self.cfg[\"l_cycles\"]):\n",
        "                z_l, z_h = self._forward_once(idx, z_h, z_l)\n",
        "            x = self.ln_f(z_h)\n",
        "            logits = self.lm_head(x)\n",
        "            if targets is not None:\n",
        "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "                losses.append(loss)\n",
        "            z_h = z_h.detach()\n",
        "            z_l = z_l.detach()\n",
        "        loss = None\n",
        "        if losses:\n",
        "            loss = sum(losses) / len(losses)\n",
        "        return logits, loss\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = TRMLanguageModel(model_config).to(device)\n",
        "print(\"param_count\", count_params(model))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## training loop (route b)\n",
        "prints loss, step time, and tokens/sec for quick feedback.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "processed_dir = Path(\"../data/processed\")\n",
        "shards = sorted(processed_dir.glob(\"stage1_shard_*.jsonl\"))\n",
        "if not shards:\n",
        "    raise FileNotFoundError(\"no processed shards found\")\n",
        "\n",
        "def load_tokens(max_tokens):\n",
        "    tokens = []\n",
        "    for shard in shards:\n",
        "        with open(shard, \"r\") as f:\n",
        "            for line in f:\n",
        "                text = json.loads(line).get(\"text\")\n",
        "                if text is None:\n",
        "                    continue\n",
        "                if not isinstance(text, str):\n",
        "                    text = str(text)\n",
        "                tokens.extend(enc.encode(text))\n",
        "                if max_tokens and len(tokens) >= max_tokens:\n",
        "                    return torch.tensor(tokens[:max_tokens], dtype=torch.long)\n",
        "    return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "token_data = load_tokens(train_config[\"max_tokens\"])\n",
        "\n",
        "def get_batch():\n",
        "    block_size = model_config[\"block_size\"]\n",
        "    batch_size = train_config[\"batch_size\"]\n",
        "    idx = torch.randint(0, token_data.numel() - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([token_data[i : i + block_size] for i in idx])\n",
        "    y = torch.stack([token_data[i + 1 : i + block_size + 1] for i in idx])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=train_config[\"lr\"])\n",
        "model.train()\n",
        "for step in range(1, train_config[\"max_steps\"] + 1):\n",
        "    t0 = time.time()\n",
        "    x, y = get_batch()\n",
        "    logits, loss = model(x, y)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    dt = time.time() - t0\n",
        "    tokens_per_sec = (x.numel()) / dt\n",
        "    if step % train_config[\"log_interval\"] == 0 or step == 1:\n",
        "        print(f\"step {step} loss {loss.item():.4f} step_time {dt:.3f}s tok/s {tokens_per_sec:.1f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "smol-llm",
      "language": "python",
      "name": "smol-llm"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}